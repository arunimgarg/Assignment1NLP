# -*- coding: utf-8 -*-
"""Assignment1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ZL8CYHNCs6redaH3tw1q5uZghUeScDNv
"""

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
from sklearn.model_selection import train_test_split
import numpy as np

dataset = pd.read_csv('/content/housing.csv')
dataset = dataset.dropna() #dropping any rows that contain NA values

print("Here are the first ten rows of the dataset: ")
dataset.head(10) #head method with argument '10' to produce first ten rows

import matplotlib.pyplot as plt
#plotting the dataset with a subplot for each variable
axes = dataset.plot.line(subplots=True,figsize=(10,10))

#setting the variable median_house_value as the label for the model
Y = dataset['median_house_value']

#setting all the other variables as features to be trained 
X = dataset.loc[:, 'longitude': 'median_income']

#splitting the data into train and test using sklearn library
#Train, test split = 70/30 with a random state of 2003 for the shuffling
x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size = 0.3,
                                                    random_state = 2003)

#convering the testing and training arrays to numpy arrays
x_train_np = x_train.to_numpy()
y_train_np = y_train.to_numpy()

x_test_np = x_test.to_numpy()
y_test_np = y_test.to_numpy()



import torch
import torch.nn as nn
import torch.nn.functional as F

from torch.nn import Conv1d
from torch.nn import MaxPool1d
from torch.nn import Flatten
from torch.nn import Linear
from torch.nn.functional import relu

from torch.utils.data import DataLoader, TensorDataset

#creating the network
class CnnRegressor(torch.nn.Module):
  def __init__(self, batch_size, inputs, outputs):
    super(CnnRegressor, self).__init__()
    self.batch_size = batch_size
    self.inputs = inputs
    self.outputs = outputs

    self.input_bn = nn.BatchNorm1d(8) #batch normalization
    
    self.input_layer = Conv1d(inputs, batch_size, 1) #input layer

    self.max_pooling_layer = MaxPool1d(1) #max-pooling layer

    self.conv_layer1 = Conv1d(batch_size, 128, 1)    #first conv layer

    self.conv_bn1 = nn.BatchNorm1d(128) #batch normaliation after first conv

    self.conv_layer2 = Conv1d(128, 256, 1) #second conv layer 

    self.conv_bn2 = nn.BatchNorm1d(256)  #batch normaliation after second conv

    self.flatten_layer = Flatten() #flatten layer to vectorize the data

    self.linear_layer = Linear(256, 64) #linear regression

    self.output_layer = Linear(64, outputs) #output layer

  def feed(self, input):
    input = input.reshape((self.batch_size, self.inputs, 1))

    output = relu(self.input_bn(input))

    output = relu(self.input_layer(input))

    output = relu(self.conv_layer1(output))

    output = relu(self.conv_bn1(output))

    output = relu(self.conv_layer2(output))

    output = relu(self.conv_bn2(output))

    output = self.max_pooling_layer(output)
    
    output = self.flatten_layer(output)

    output = self.linear_layer(output)

    output = self.output_layer(output)

    return output

from torch.optim import Adam
from torch.nn import L1Loss

!pip install pytorch-ignite
from ignite.contrib.metrics.regression.r2_score import R2Score

#initializing the model and variables
batch_size = 64

model = CnnRegressor(batch_size, X.shape[1], 1)

model.cuda()

#for printing the number of trainable variables
def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad) 
    #numel(counts the number of elements)

#train and test model and return L1 loss and R^2 score

def model_loss(model, dataset, train = False, optimizer = None):
  performance = L1Loss()
  score_metric = R2Score()

  avg_loss = 0
  avg_score = 0
  count = 0

  for input, output in iter(dataset):
    predictions = model.feed(input)

    loss = performance(predictions, output)

    score_metric.update([predictions, output])
    score = score_metric.compute()

    if(train):
      optimizer.zero_grad()

      loss.backward()

      optimizer.step()

    avg_loss += loss.item()
    avg_score += score
    count += 1

  return avg_loss / count, avg_score / count

import time
#initializing the paramteres for the model and training the model
epochs = 150

optimizer = Adam(model.parameters(), lr = 1E-2)

inputs = torch.from_numpy(x_train_np).cuda().float()
outputs = torch.from_numpy(y_train_np.reshape(y_train_np.shape[0], 1)).cuda().float()

tensor = TensorDataset(inputs, outputs)
loader = DataLoader(tensor, batch_size, shuffle=True, drop_last=True)

start = time.time()

for epoch in range(epochs):
  avg_loss, avg_r2_score = model_loss(model, loader, train=True, optimizer = optimizer)

  print("Epoch " + str(epoch+1) + ":\n\tLoss = " + str(avg_loss) + "\n\tR^2 Score = " + str(avg_r2_score))

end = time.time()

torch.cuda.synchronize()

print("Training time: ", "{:.2f}".format(end-start))

torch.save(model.state_dict(), '1092641_1dconv_reg.pt')

#testing the model
inputs = torch.from_numpy(x_test_np).cuda().float()
outputs = torch.from_numpy(y_test_np.reshape(y_test_np.shape[0], 1)).cuda().float()

tensor = TensorDataset(inputs, outputs)
loader = DataLoader(tensor, batch_size, shuffle = True, drop_last = True)
start = time.time()
avg_loss, avg_r2_score = model_loss(model, loader)
end = time.time()

torch.cuda.synchronize()


print("The model's L1 loss is: " + str(avg_loss))
print("The model's R^2 score isL " + str(avg_r2_score))
print("Testing time: ", "{:.2f}".format(end-start))
print("Number of trainable parameters: ", count_parameters(model))